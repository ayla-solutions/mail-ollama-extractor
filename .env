# =========================
# Ollama connection
# =========================
OLLAMA_HOST=http://ollama-mail-statementapi.australiaeast.azurecontainer.io:11434

# Model names
CLASSIFIER_MODEL=mail-classifier-small
INVOICE_MODEL=invoice-extractor-small
REQUEST_MODEL=request-summarizer-small

# LLM tuning
# Lower prediction budget for classify, higher for extraction
OLLAMA_TEMPERATURE=0.2
OLLAMA_NUM_PREDICT=160
OLLAMA_NUM_CTX=3072
OLLAMA_KEEP_ALIVE=30m

# Specific caps
INVOICE_NUM_PREDICT=400
REQUEST_NUM_PREDICT=160
CLASSIFY_NUM_PREDICT=120

# =========================
# Runtime behavior
# =========================
# Hard cap on input size sent to LLM (chars)
EXTRACTOR_MAX_CHARS=12000
# Smaller cap only for classification stage
CLASSIFY_MAX_CHARS=4000

# Deterministic ticketing
TICKET_PREFIX=CR-
TICKET_DETERMINISTIC=True

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Slow-call thresholds (ms)
SLOW_CLASSIFY_WARN_MS=15000
SLOW_INVOICE_WARN_MS=20000
SLOW_REQUEST_WARN_MS=12000

# Keep classification snappy; extraction can be longer.
CLASSIFY_MAX_CHARS=4000

# Prefer a larger context for invoices/requests if your Ollama host allows it.
OLLAMA_NUM_CTX=3072

# Structured logs: "json" or "plain"
LOG_FORMAT=json
